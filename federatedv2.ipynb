{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from lenet5 import LeNet\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters \n",
    "\n",
    "# Check if cuda is available\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model Training Parameters \n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-3 \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Parameters for Federated :earning\n",
    "num_clients = 20 # number of total clients\n",
    "num_selected = 6 # number of  clients selected for the training \n",
    "num_rounds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X, y_target in train_loader:\n",
    "        \n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if there is a GPU\n",
    "\n",
    "        X = X.to(device)\n",
    "        y_target =y_target.to(device)\n",
    "\n",
    "        # prediction\n",
    "\n",
    "        # call model forward()\n",
    "        y_predict, _ = model(X)\n",
    "        # get loss\n",
    "        loss = criterion(y_predict, y_target)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        \n",
    "        # adjusting weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss\n",
    " \n",
    "def test(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    for X, y_target in valid_loader:\n",
    "        # if there is a GPU\n",
    "\n",
    "        X = X.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "\n",
    "        # prediction and loss\n",
    "\n",
    "        # call model forward()\n",
    "        y_predict, _ = model(X)\n",
    "        # get loss\n",
    "        loss = criterion(y_predict, y_target)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "    return model, epoch_loss\n",
    " \n",
    "\n",
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n\n",
    " \n",
    "def client_update(model, optimizer, train_loader,device,criterion ,epoch=5):\n",
    "    \"\"\"\n",
    "    This function updates/trains client model on client data\n",
    "    \"\"\"\n",
    "    # model.train()\n",
    "    for e in range(epoch):\n",
    "        model, optimizer, train_loss = train(train_loader, model,\n",
    "                                criterion, optimizer, device)\n",
    "\n",
    "\n",
    "\n",
    "        # for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #     # data, target = data, target\n",
    "        #     optimizer.zero_grad()\n",
    "        #     output = model(data)\n",
    "        #     loss = F.nll_loss(output, target) # The negative log likelihood loss\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "    # return loss.item()\n",
    "    #print(train_loss)\n",
    "    return train_loss\n",
    "\n",
    "def server_aggregate(global_model, client_models):\n",
    "    \"\"\"\n",
    "    This function has aggregation method 'mean'\n",
    "    \"\"\"\n",
    "    ### This will take simple mean of the weights of models ###\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "        # print(global_dict[k])\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(centralizedModel, federatedModels, criterion, optimizers, train_loader, test_loader,\n",
    "                epochs, device, print_every=1):\n",
    "\n",
    "    global_train_losses = [] # train_loss\n",
    "    global_valid_losses = []\n",
    "\n",
    "    global_train_accuracies = [] # train_accuracy\n",
    "    global_valid_accuracies = []\n",
    "\n",
    "    for round in range(num_rounds):\n",
    "\n",
    "        # Select random clients\n",
    "        # Select in the total number of clients, a random array of clients of size num_selected at each round\n",
    "        client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    \n",
    "        local_train_losses = [] # local_losses\n",
    "        local_train_accuracies = []\n",
    "        local_valid_losses = []\n",
    "        local_valid_accuracies = []\n",
    "\n",
    "        for i in range(num_selected):\n",
    "            # Train federated model locally in client i for num epochs = EPOCHS\n",
    "            local_train_loss = client_update(federatedModels[i], optimizers[i], train_loader[client_idx[i]],DEVICE,criterion, epoch=EPOCHS)\n",
    "            local_train_acc = get_accuracy(federatedModels[i], train_loader[client_idx[i]], DEVICE)\n",
    "\n",
    "            local_train_losses.append(local_train_loss)\n",
    "            local_train_accuracies.append(local_train_acc)\n",
    "\n",
    "            local_valid_loss = test(test_loader, federatedModels[i], criterion, DEVICE)[1]\n",
    "            local_valid_acc = get_accuracy(federatedModels[i], test_loader, DEVICE)\n",
    "\n",
    "            local_valid_losses.append(local_valid_loss)\n",
    "            local_valid_accuracies.append(local_valid_acc)\n",
    "\n",
    "        server_aggregate(centralizedModel, federatedModels)\n",
    "\n",
    "        # Calculate avg training loss over all selected users at each round\n",
    "        local_train_loss_avg = sum(local_train_losses) / len(local_train_losses)\n",
    "        global_train_losses.append(local_train_loss_avg)\n",
    "\n",
    "        # Calculate avg training accuracy over all selected users at each round \n",
    "        local_train_acc_avg = sum(local_train_accuracies) / len(local_train_accuracies)\n",
    "        global_train_accuracies.append(local_train_acc_avg)\n",
    "\n",
    "\n",
    "        # Calculate avg valid loss over all selected users at each round\n",
    "        local_valid_loss_avg = sum(local_valid_losses) / len(local_valid_losses)\n",
    "        global_valid_losses.append(local_valid_loss_avg)\n",
    "\n",
    "        # Calculate avg valid accuracy over all selected users at each round \n",
    "        local_valid_acc_avg = sum(local_valid_accuracies) / len(local_valid_accuracies)\n",
    "        global_valid_accuracies.append(local_valid_acc_avg)\n",
    "\n",
    "\n",
    "\n",
    "        # valid_loss = test(test_loader,centralizedModel,criterion, DEVICE)[1] # Test global model on data\n",
    "        #valid_acc = get_accuracy(centralizedModel, test_loader, DEVICE)\n",
    "    \n",
    "        #global_valid_losses.append(valid_loss)\n",
    "        #global_valid_accuracies.append(valid_acc)\n",
    "\n",
    "        #print('%d-th round' % r)\n",
    "        # print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, test_acc))\n",
    "        #print('average train loss %0.3g ' % (loss / num_selected))\n",
    "        #print(test_loss)\n",
    "        #print(type(test_loss))\n",
    "        #print(' test loss %0.3g '%(test_loss))\n",
    "        #print('test acc: %0.3f'% (test_acc))\n",
    "        print(f'Round: {round}\\t'\n",
    "                f'Train loss: {local_train_loss_avg:.4f}\\t'\n",
    "                  f'Valid loss: {local_valid_loss_avg:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * local_train_acc_avg:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * local_valid_acc_avg:.2f}')\n",
    "\n",
    "\n",
    "\n",
    "        # train_acc = get_accuracy(model, train_loader, device)\n",
    "        # test_acc = get_accuracy(model, test_loader, device)\n",
    "        # DOES NOT GET THE ACCURACY, CHECK WITH THE FUNCITON FROM THE FEDERATED LEARNING AAND COMPARE WITH CENTRALISED MODEL\n",
    "    \n",
    "    return centralizedModel, federatedModels, optimizers, (global_train_losses, global_valid_losses), (global_train_accuracies, global_valid_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders and Transformations\n",
    "\n",
    "# Image augmentation \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalizing the test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Loading CIFAR10 using torchvision.datasets\n",
    "traindata = datasets.CIFAR10('./data', train=True, download=False, \n",
    "                transform=transform_train)\n",
    "\n",
    "\n",
    "# Dividing the training data into num_clients, with each client having equal number of images\n",
    "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] \n",
    "                    / num_clients) for _ in range(num_clients)])\n",
    "                    \n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=BATCH_SIZE, shuffle=True) for x in traindata_split]\n",
    "\n",
    "\n",
    "# Loading the test iamges and thus converting them into a test_loader\n",
    "test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./data', train=False, \n",
    "            transform=transforms.Compose([transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "            ), batch_size=BATCH_SIZE, shuffle=True)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "model = LeNet().to(DEVICE)\n",
    "centralizedModel = model\n",
    "\n",
    "# list of models, model per device SELECTED ( same model for each device in our case)\n",
    "federatedModels =  [model for _ in range(num_selected)]\n",
    "\n",
    "for models in federatedModels:\n",
    "    models.load_state_dict(centralizedModel.state_dict())  # we initialize every model with the central\n",
    "\n",
    "\n",
    "optimizers = [torch.optim.SGD(model.parameters(), lr=LEARNING_RATE) for model in federatedModels]\n",
    "# NO CRITERION?\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "centralizedModel, federatedModels, optimizers, (train_losses, valid_losses), (train_accuracies, valid_accuracies) = training_loop(centralizedModel, federatedModels, criterion, optimizers, train_loader, test_loader, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting Accuracy and Loss\n",
    "\n",
    "SAVE_PLOTS = True # To save plots or show them\n",
    "if SAVE_PLOTS:\n",
    "    matplotlib.use(\"Agg\") \n",
    "\n",
    "# Plot Loss \n",
    "plt.figure()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(range(len(train_losses)), train_losses, color=\"r\", label=\"Training\")\n",
    "plt.plot(range(len(valid_losses)), valid_losses, color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "if SAVE_PLOTS:\n",
    "    plt.savefig(\"plots/centralised_LR[{}]_EPOCHS[{}]_loss.png\".format(LEARNING_RATE, EPOCHS)) \n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure()\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.plot(range(len(train_accuracies)), train_accuracies, color=\"r\", label=\"Training\")\n",
    "plt.plot(range(len(valid_accuracies)), valid_accuracies, color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "if SAVE_PLOTS:\n",
    "    plt.savefig(\"plots/federated_LR[{}]_EPOCHS[{}]_accuracy.png\".format(LEARNING_RATE, EPOCHS))\n",
    "else:\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a605e0aa9ba0153468378976782e6a438b220bce716ba7043b32bc2b74e9bc8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
