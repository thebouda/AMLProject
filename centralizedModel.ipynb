{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import fedAvg\n",
    "from torchvision import datasets, transforms\n",
    "from lenet5 import LeNet\n",
    "from datetime import datetime\n",
    "\n",
    "# check for cuda\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-2\n",
    "N_CLIENTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X, y_target in train_loader:\n",
    "        \n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if there is a GPU\n",
    "\n",
    "        X = X.to(device)\n",
    "        y_target =y_target.to(device)\n",
    "\n",
    "        # prediction\n",
    "\n",
    "        # call model forward()\n",
    "        y_predict, _ = model(X)\n",
    "        # get loss\n",
    "        loss = criterion(y_predict, y_target)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        \n",
    "        # adjusting weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss\n",
    "\n",
    "def test(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    for X, y_target in valid_loader:\n",
    "        # if there is a GPU\n",
    "\n",
    "        X = X.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "\n",
    "        # prediction and loss\n",
    "\n",
    "        # call model forward()\n",
    "        y_predict, _ = model(X)\n",
    "        # get loss\n",
    "        loss = criterion(y_predict, y_target)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "    return model, epoch_loss\n",
    "\n",
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n\n",
    "\n",
    "def training_loop(model, criterion, optimizer, train_loader, test_loader,\n",
    "                epochs, device, print_every=1):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(epochs):\n",
    "        model, optimizer, train_loss = train(train_loader, model,\n",
    "                                criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validation\n",
    "\n",
    "        # disable gradient calculation to save memory\n",
    "        with torch.no_grad():\n",
    "            model, valid_loss = test(test_loader, model, criterion, device)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            train_acc = get_accuracy(model, train_loader, device)\n",
    "            test_acc = get_accuracy(model, test_loader, device)\n",
    "\n",
    "            print(f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * test_acc:.2f}')\n",
    "\n",
    "    return model, optimizer, (train_losses, valid_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Image augmentation \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalizing the test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Loading CIFAR10 using torchvision.datasets\n",
    "#traindata = datasets.CIFAR10('./data', train=True, download=False,\n",
    "#                       transform= transform_train)\n",
    "traindata = datasets.CIFAR10('./data', train=True, download=False, \n",
    "                transform=transform_train)\n",
    "\n",
    "# # Dividing the training data into num_clients, with each client having equal number of images\n",
    "# traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] \n",
    "#                     / num_clients) for _ in range(num_clients)])\n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "train_loader = torch.utils.data.DataLoader(traindata, batch_size=BATCH_SIZE, shuffle=True) \n",
    "\n",
    "# Loading the test iamges and thus converting them into a test_loader\n",
    "test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./data', train=False, \n",
    "            transform=transforms.Compose([transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "            ), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = LeNet().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.nll_loss()\n",
    "\n",
    "model, optimizer, (train_losses, valid_losses) = training_loop(model, criterion, optimizer,\n",
    "                        train_loader, test_loader, EPOCHS, DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
