{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from lenet5 import LeNet\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cuda is available\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model Training Parameters\n",
    "\n",
    "# Hyperparameter of FedAvgM\n",
    "BETA = 0.9\n",
    "GLOBAL_LEARNING_RATE = 1 # Learning rate of the global model (server)\n",
    "\n",
    "LEARNING_RATE = 2e-3  # LR\n",
    "BATCH_SIZE = 32\n",
    "ROUNDS = 20  # R\n",
    "LOCAL_EPOCHS = 5  # E\n",
    "NUM_CLIENTS = 100  # K: number of total clients\n",
    "C = 0.3 # percentage of clients selected at each round\n",
    "# m = C * K : number of  clients selected at each round\n",
    "NUM_SELECTED = max(int(C * NUM_CLIENTS), 1)\n",
    "\n",
    "# Save plots in the folder ./plots or show them\n",
    "SAVE_PLOTS = False\n",
    "\n",
    "# Dirichlet's distribution\n",
    "# NUM_CLIENTS must be = 100\n",
    "DIRICHLET = True \n",
    "# Alpha value for the dirichlet's distribution\n",
    "# Value possible for CIFAR10  : 0, 0.05, 0.1, 0.2, 0.5, 1, 10, 100\n",
    "# Value possible for CIFAR100 : 0, 0.5, 1, 2, 5, 10, 100, 1000\n",
    "ALPHA = 0.20\n",
    "\n",
    "# If the clients have different numbers of images or not\n",
    "DIFFERENT_SIZES = False \n",
    "DELTA = -1 # controls how much the images numbers can vary from client to client\n",
    "\n",
    "# Use batch normalization or not\n",
    "BATCH_NORM = False \n",
    "# group normalization\n",
    "GROUP_NORM = False \n",
    "\n",
    "# group normalization parameters\n",
    "groupNormParams= {\n",
    "'groupNL1' : 2,\n",
    "'groupNL2' : 2\n",
    "}\n",
    "\n",
    "\n",
    "if GROUP_NORM ==True & BATCH_NORM ==True:\n",
    "    print(\" Cannot have group an batch normalization True at the same time\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X, y_target in train_loader:\n",
    "\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # If there is a GPU, pass the data to GPU\n",
    "        X = X.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "\n",
    "        # Prediction\n",
    "\n",
    "        # Call model forward()\n",
    "        y_predict, _ = model(X)\n",
    "\n",
    "        # Get loss\n",
    "        loss = criterion(y_predict, y_target)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "        # Adjusting weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss\n",
    "\n",
    "\n",
    "def test(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    for X, y_target in valid_loader:\n",
    "\n",
    "        # If there is a GPU, pass the data to the GPU\n",
    "        X = X.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "\n",
    "        # Prediction and loss\n",
    "\n",
    "        # Call model forward()\n",
    "        y_predict, _ = model(X)\n",
    "\n",
    "        # Get loss\n",
    "        loss = criterion(y_predict, y_target)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "    return model, epoch_loss\n",
    "\n",
    "\n",
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "\n",
    "    correct_pred = 0\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n\n",
    "\n",
    "\n",
    "def client_update(model, optimizer, train_loader, device, criterion, epochs):\n",
    "    \"\"\"\n",
    "    This function updates/trains client model on client data\n",
    "    \"\"\"\n",
    "    for e in range(epochs):\n",
    "        model, optimizer, train_loss = train(train_loader, model,\n",
    "                                             criterion, optimizer, device)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def server_aggregate(global_model, client_models, client_idx, lengths, global_optimizer):\n",
    "    \"\"\"\n",
    "    Server aggregation function using FedAvgM algorithm with server Momentum \n",
    "    \"\"\"\n",
    "    # This will take simple mean of the weights of models\n",
    "\n",
    "    totLength= float(sum(lengths))\n",
    "\n",
    "    pseudo_gradient_clients = []\n",
    "    # For each client calculate the pseudo-gradient, and add it to the list\n",
    "    for i in range(len(lengths)):\n",
    "        # It's the update of the client, given by the difference of its state dict and the server state dict\n",
    "        pseudo_gradient_clients.append({key: client_models[client_idx[i]].state_dict()[key] - global_model.state_dict()[key] for key in client_models[client_idx[i]].state_dict()})\n",
    "            \n",
    "    # Do vanilla FedAvg on the pseudo gradients of the clients and get the final pseudo gradient\n",
    "    pseudo_gradient = None \n",
    "    for i in range(len(lengths)):\n",
    "        if i == 0: # If first client, initialize the pseudo gradient with the one of the pseudo client, weighted accordingly\n",
    "            pseudo_gradient = {key: pseudo_gradient_clients[i][key] * float(lengths[i] / totLength) for key in pseudo_gradient_clients[i]}\n",
    "        else:\n",
    "            pseudo_gradient = {key: pseudo_gradient[key] + pseudo_gradient_clients[i][key] * float(lengths[i] / totLength) for key in pseudo_gradient_clients[i]}\n",
    "\n",
    "\n",
    "    # Now we have calculated the final pseudo gradient : update the gradient of the global model\n",
    "    for n, p in global_model.named_parameters():\n",
    "        p.grad = -1.0 * pseudo_gradient[n]\n",
    "    \n",
    "    # Update the model with the optimizer formula\n",
    "    global_optimizer.step()\n",
    "\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(centralizedModel, federatedModels, criterion, global_optimizer, optimizers, train_loader, test_loader,\n",
    "                  rounds, epochs, num_clients, num_selected, device, print_every=1):\n",
    "\n",
    "    avg_train_losses = []  # weighted average train losses between clients\n",
    "    avg_train_accuracies = []  # weighted average train accuracies between clients\n",
    "\n",
    "    valid_losses = []   \n",
    "    valid_accuracies = []\n",
    "\n",
    "    global_optimizer.zero_grad()\n",
    "\n",
    "    # Train model\n",
    "    for round in range(rounds):\n",
    "\n",
    "        # Select random clients\n",
    "        # Select in the total number of clients, a random array of clients of size num_selected at each round\n",
    "        client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "\n",
    "        local_train_losses = []  # Local train losses of the clients in this round\n",
    "        local_train_accuracies = []  # Local train accuracies of the clients in this round\n",
    "\n",
    "        local_len = []\n",
    "        tot_images = 0\n",
    "\n",
    "        for i in range(num_selected):\n",
    "            # Train federated model locally in client i for num_epochs epochs\n",
    "            local_train_loss = client_update(federatedModels[client_idx[i]], optimizers[client_idx[i]], train_loader[client_idx[i]], device, criterion, epochs)\n",
    "            local_train_acc = get_accuracy(federatedModels[client_idx[i]], train_loader[client_idx[i]], device)\n",
    "\n",
    "            lenDataLoad = len(train_loader[client_idx[i]]) # number of images seen by that client\n",
    "            tot_images += lenDataLoad\n",
    "            local_len.append(lenDataLoad) # gets the number of images per data loader\n",
    "\n",
    "            local_train_losses.append(local_train_loss * lenDataLoad) # Add the local loss for this client weighted with the number of images seen\n",
    "            local_train_accuracies.append(local_train_acc * lenDataLoad)\n",
    "\n",
    "        server_aggregate(centralizedModel, federatedModels, client_idx, local_len, global_optimizer)\n",
    "\n",
    "        # Calculate avg training loss over all selected users at each round\n",
    "        avg_train_loss= sum(local_train_losses) / tot_images \n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Calculate avg training accuracy over all selected users at each round\n",
    "        avg_train_acc = sum(local_train_accuracies) / tot_images \n",
    "        avg_train_accuracies.append(avg_train_acc)\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        # Disable gradient calculation to save memory\n",
    "        with torch.no_grad():\n",
    "            model, valid_loss = test(test_loader, centralizedModel, criterion, device)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            valid_acc = get_accuracy(centralizedModel, test_loader, device)\n",
    "            valid_accuracies.append(valid_acc)\n",
    "\n",
    "\n",
    "\n",
    "        print(f'Round: {round}\\t'\n",
    "              f'Average Train loss: {avg_train_loss:.4f}\\t'\n",
    "              f'Valid loss: {valid_loss:.4f}\\t'\n",
    "              f'Average Train accuracy: {100 * avg_train_acc:.2f}\\t'\n",
    "              f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "    return centralizedModel, federatedModels, optimizers, (avg_train_losses, valid_losses), (avg_train_accuracies, valid_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet's Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet(dataset, alpha):\n",
    "    # CIFAR10\n",
    "    train_file_name = \"cifar10_v1.1/federated_train_alpha_\"\n",
    "    # test_file_name = \"cifar10_v1.1/test.csv\" # NOT USED\n",
    "\n",
    "    # CIFAR100\n",
    "    # split_file_name = \"cifar100_v1.1/federated_train_alpha_\"\n",
    "    #test_file_name = \"cifar100_v1.1/test.csv\"\n",
    "    \n",
    "    if 0 < alpha < 0.1:\n",
    "        train_file_name += str(alpha) +\".csv\"\n",
    "    elif 0 < alpha < 1:\n",
    "        train_file_name += str(alpha) +\"0.csv\"\n",
    "    else:\n",
    "        train_file_name += str(alpha) + \".00.csv\"\n",
    "    train_file = pd.read_csv(train_file_name)\n",
    "    # test_file = pd.read_csv(test_file_name)\n",
    "    # print(test_file)\n",
    "\n",
    "    client_img_index = [[] for _ in range(100)]\n",
    "    for i,client_id in enumerate(train_file['user_id']):\n",
    "        client_img_index[client_id].append(train_file['image_id'][i])\n",
    "\n",
    "    traindata_split = [torch.utils.data.Subset(dataset, client_img_index[i]) for i in range(100)]\n",
    "    return traindata_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirichlet with alpha: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Image augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalizing the test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "# Loading CIFAR10 using torchvision.datasets\n",
    "traindata = datasets.CIFAR10('./data', train=True, download=False,\n",
    "                             transform=transform_train)\n",
    "\n",
    "\n",
    "total_data = traindata.data.shape[0] # number of data\n",
    "\n",
    "if DIRICHLET:\n",
    "    print(f\"Dirichlet with alpha: {ALPHA}\")\n",
    "    traindata_split = dirichlet(traindata, ALPHA)\n",
    "\n",
    "elif DIFFERENT_SIZES:\n",
    "    # Dividing the training data into num_clients, with each clients having different number of images\n",
    "    min_val = max(int(total_data/ NUM_CLIENTS) - DELTA, 1) # min value of number of images for each client\n",
    "    max_val = min(int(total_data/ NUM_CLIENTS) + DELTA, total_data - 1) # max value of number of images for each client\n",
    "\n",
    "    indices = list(range(NUM_CLIENTS)) # list of indices for the splits of the data\n",
    "    lengths = [random.randint(min_val,max_val) for i in indices] # List of lengths of splits to be produced\n",
    "\n",
    "    diff = sum(lengths) - total_data # we are off by this abount \n",
    "\n",
    "    # Iterate through, incrementing/decrementing a random index \n",
    "    while diff != 0:  \n",
    "        addthis = 1 if diff > 0 else -1 # +/- 1 depending on if we were above or below target.\n",
    "        diff -= addthis\n",
    "\n",
    "        idx = random.choice(indices) # Pick a random index to modify, check if it's OK to modify\n",
    "        while not (min_val < (lengths[idx] - addthis) < max_val): \n",
    "            idx = random.choice(indices) # Not OK to modify.  Pick another.\n",
    "\n",
    "        lengths[idx] -= addthis #Update that index.\n",
    "    \n",
    "    print(\"Number of Images for each client:\")\n",
    "    print(lengths)\n",
    "    \n",
    "    traindata_split = torch.utils.data.random_split(traindata, lengths)\n",
    "\n",
    "else:\n",
    "    delta = -1 # Not used, just to print it\n",
    "    # Dividing the training data into num_clients, with each client having equal number of images\n",
    "    traindata_split = torch.utils.data.random_split(traindata, [int(total_data/ NUM_CLIENTS) for _ in range(NUM_CLIENTS)])\n",
    "\n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "train_loader = [torch.utils.data.DataLoader(\n",
    "    x, batch_size=BATCH_SIZE, shuffle=True) for x in traindata_split]\n",
    "\n",
    "\n",
    "# Loading the test iamges and thus converting them into a test_loader\n",
    "test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./data', train=False,\n",
    "                                                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "                                                           ), batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(BATCH_NORM,GROUP_NORM,groupNormParams).to(DEVICE)\n",
    "centralizedModel = model\n",
    "\n",
    "# list of models, model per num_clients devices ( same model for each device in our case)\n",
    "federatedModels = [model for _ in range(NUM_CLIENTS)]\n",
    "\n",
    "for models in federatedModels:\n",
    "    # we initialize every model with the central\n",
    "    models.load_state_dict(centralizedModel.state_dict())\n",
    "\n",
    "\n",
    "optimizers = [torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "              for model in federatedModels]\n",
    "global_optimizer = torch.optim.SGD(model.parameters(), lr=GLOBAL_LEARNING_RATE, momentum=BETA)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0\tAverage Train loss: 0.3244\tValid loss: 5.3736\tAverage Train accuracy: 87.73\tValid accuracy: 10.00\n",
      "Round: 1\tAverage Train loss: 0.2760\tValid loss: 6.6429\tAverage Train accuracy: 90.61\tValid accuracy: 10.00\n",
      "Round: 2\tAverage Train loss: 0.2829\tValid loss: 4.3273\tAverage Train accuracy: 89.81\tValid accuracy: 10.00\n",
      "Round: 3\tAverage Train loss: 0.3049\tValid loss: 5.2745\tAverage Train accuracy: 88.97\tValid accuracy: 10.00\n",
      "Round: 4\tAverage Train loss: 0.2712\tValid loss: 4.3246\tAverage Train accuracy: 91.02\tValid accuracy: 13.53\n",
      "Round: 5\tAverage Train loss: 0.2289\tValid loss: 5.4006\tAverage Train accuracy: 92.08\tValid accuracy: 10.46\n",
      "Round: 6\tAverage Train loss: 0.1648\tValid loss: 5.5463\tAverage Train accuracy: 94.05\tValid accuracy: 10.00\n",
      "Round: 7\tAverage Train loss: 0.1478\tValid loss: 4.6241\tAverage Train accuracy: 96.23\tValid accuracy: 12.11\n",
      "Round: 8\tAverage Train loss: 0.2098\tValid loss: 4.3169\tAverage Train accuracy: 92.99\tValid accuracy: 17.37\n",
      "Round: 9\tAverage Train loss: 0.2074\tValid loss: 4.6195\tAverage Train accuracy: 92.70\tValid accuracy: 17.89\n",
      "Round: 10\tAverage Train loss: 0.2181\tValid loss: 5.5855\tAverage Train accuracy: 92.38\tValid accuracy: 10.06\n",
      "Round: 11\tAverage Train loss: 0.1678\tValid loss: 5.6430\tAverage Train accuracy: 95.06\tValid accuracy: 11.82\n",
      "Round: 12\tAverage Train loss: 0.2109\tValid loss: 4.4825\tAverage Train accuracy: 92.61\tValid accuracy: 17.64\n",
      "Round: 13\tAverage Train loss: 0.1991\tValid loss: 5.2805\tAverage Train accuracy: 93.15\tValid accuracy: 11.37\n",
      "Round: 14\tAverage Train loss: 0.1678\tValid loss: 3.4997\tAverage Train accuracy: 94.22\tValid accuracy: 21.51\n",
      "Round: 15\tAverage Train loss: 0.1735\tValid loss: 5.8762\tAverage Train accuracy: 94.35\tValid accuracy: 10.90\n",
      "Round: 16\tAverage Train loss: 0.2061\tValid loss: 3.5772\tAverage Train accuracy: 93.45\tValid accuracy: 12.09\n",
      "Round: 17\tAverage Train loss: 0.1674\tValid loss: 4.7920\tAverage Train accuracy: 94.87\tValid accuracy: 12.08\n",
      "Round: 18\tAverage Train loss: 0.2276\tValid loss: 2.9026\tAverage Train accuracy: 92.41\tValid accuracy: 22.17\n",
      "Round: 19\tAverage Train loss: 0.2156\tValid loss: 3.3334\tAverage Train accuracy: 92.41\tValid accuracy: 21.01\n"
     ]
    }
   ],
   "source": [
    "centralizedModel, federatedModels, optimizers, (train_losses, valid_losses), (train_accuracies, valid_accuracies) = training_loop(\n",
    "    centralizedModel, federatedModels, criterion, global_optimizer, optimizers, train_loader, test_loader, ROUNDS, LOCAL_EPOCHS, NUM_CLIENTS, NUM_SELECTED, DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PLOTS = False\n",
    "if SAVE_PLOTS:\n",
    "    matplotlib.use(\"Agg\")\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(range(len(train_losses)), train_losses, color=\"r\", label=\"Training\")\n",
    "plt.plot(range(len(valid_losses)), valid_losses, color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "if SAVE_PLOTS:\n",
    "    if DIRICHLET:\n",
    "        plt.savefig(\"plots/federated_FedAvgM_B=[{}]_GLR=[{}]_LR[{}]_R[{}]_K[{}]_C[{}]_E[{}]_A[{}]_BATCHNORM[{}]_GROUPNORM[{},{},{}]_loss.png\".format(\n",
    "            BETA, GLOBAL_LEARNING_RATE,LEARNING_RATE, ROUNDS, NUM_CLIENTS, C, LOCAL_EPOCHS, ALPHA, BATCH_NORM, GROUP_NORM, groupNormParams['groupNL1'], groupNormParams['groupNL2']))\n",
    "    else:\n",
    "        plt.savefig(\"plots/federated_FedAvgM_B=[{}]_GLR=[{}]_LR[{}]_R[{}]_K[{}]_C[{}]_E[{}]_D[{}]_BATCHNORM[{}]_GROUPNORM[{},{},{}]_loss.png\".format(\n",
    "            BETA, GLOBAL_LEARNING_RATE,LEARNING_RATE, ROUNDS, NUM_CLIENTS, C, LOCAL_EPOCHS, delta, BATCH_NORM, GROUP_NORM, groupNormParams['groupNL1'], groupNormParams['groupNL2']))\n",
    "else:\n",
    "    plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure()\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.plot(range(len(train_accuracies)),\n",
    "         train_accuracies, color=\"r\", label=\"Training\")\n",
    "plt.plot(range(len(valid_accuracies)), valid_accuracies,\n",
    "         color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "if SAVE_PLOTS:\n",
    "    if DIRICHLET:\n",
    "        plt.savefig(\"plots/federated_FedAvgM_B=[{}]_GLR=[{}]_LR[{}]_R[{}]_K[{}]_C[{}]_E[{}]_A[{}]_BATCHNORM[{}]_GROUPNORM[{},{},{}]_accuracy.png\".format(\n",
    "            BETA, GLOBAL_LEARNING_RATE,LEARNING_RATE, ROUNDS, NUM_CLIENTS, C,LOCAL_EPOCHS, ALPHA, BATCH_NORM, GROUP_NORM, groupNormParams['groupNL1'], groupNormParams['groupNL2']))\n",
    "    else:\n",
    "        plt.savefig(\"plots/federated_FedAvgM_B=[{}]_GLR=[{}]_LR[{}]_R[{}]_K[{}]_C[{}]_E[{}]_D[{}]_BATCHNORM[{}]_GROUPNORM[{},{},{}]_accuracy.png\".format(\n",
    "            BETA, GLOBAL_LEARNING_RATE,LEARNING_RATE, ROUNDS, NUM_CLIENTS, C, LOCAL_EPOCHS, delta, BATCH_NORM, GROUP_NORM, groupNormParams['groupNL1'], groupNormParams['groupNL2']))\n",
    "else:\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"FedAvgMR{}E{}A{}C{}.txt\".format(ROUNDS,LOCAL_EPOCHS,ALPHA,C), \"w\") as f:\n",
    "    #for res in valid_accuracies:\n",
    "        #f.write(str(float(res)) + \" \\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a605e0aa9ba0153468378976782e6a438b220bce716ba7043b32bc2b74e9bc8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
