{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import fedAvg\n",
    "from torchvision import datasets, transforms\n",
    "from lenet5 import LeNet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm # progress bar, not really necessary\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for federated learning\n",
    "num_clients = 20 # number of total clients\n",
    "num_selected = 6 # number of  clients selected for the training \n",
    "num_rounds = 10\n",
    "\n",
    "# parameters\n",
    "EPOCHS = 10 # epochs defined by user\n",
    "LEARNING_RATE = 2e-3 \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders and transformations\n",
    "# Image augmentation \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalizing the test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Loading CIFAR10 using torchvision.datasets\n",
    "#traindata = datasets.CIFAR10('./data', train=True, download=False,\n",
    "#                       transform= transform_train)\n",
    "traindata = datasets.CIFAR10('./data', train=True, download=False, \n",
    "                transform=transform_train)\n",
    "\n",
    "\n",
    "# Dividing the training data into num_clients, with each client having equal number of images\n",
    "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "\n",
    "\n",
    "# # Dividing the training data into num_clients, with each client having equal number of images\n",
    "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] \n",
    "                    / num_clients) for _ in range(num_clients)])\n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "# train_loader = torch.utils.data.DataLoader(traindata, batch_size=BATCH_SIZE, shuffle=True) \n",
    "\n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=BATCH_SIZE, shuffle=True) for x in traindata_split]\n",
    "\n",
    "\n",
    "# Loading the test iamges and thus converting them into a test_loader\n",
    "test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./data', train=False, \n",
    "            transform=transforms.Compose([transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "            ), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE == 'cuda':\n",
    "    modelChosen = LeNet().to(DEVICE)\n",
    "else:\n",
    "    modelChosen = LeNet()\n",
    "    print(\"we're not on cuda\")\n",
    "\n",
    "\n",
    "centralizedModel = modelChosen\n",
    "\n",
    "# list of models, model per device SELECTED ( same model for each device in our case)\n",
    "federatedModels =  [modelChosen for _ in range(num_selected)]\n",
    "\n",
    "for models in federatedModels:\n",
    "    models.load_state_dict(centralizedModel.state_dict())  # we initialize every model with the central\n",
    "\n",
    "\n",
    "optimizers = [torch.optim.SGD(model.parameters(), lr=LEARNING_RATE) for model in federatedModels]\n",
    "# NO CRITERION?\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X, y_target in train_loader:\n",
    "        \n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if there is a GPU\n",
    "\n",
    "        X = X.to(device)\n",
    "        y_target =y_target.to(device)\n",
    "\n",
    "        # prediction\n",
    "\n",
    "        # call model forward()\n",
    "        y_predict, _ = model(X)\n",
    "        # get loss\n",
    "        loss = criterion(y_predict, y_target)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        \n",
    "        # adjusting weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss\n",
    "\n",
    "def client_update(model, optimizer, train_loader,device,criterion ,epoch=5):\n",
    "    \"\"\"\n",
    "    This function updates/trains client model on client data\n",
    "    \"\"\"\n",
    "    # model.train()\n",
    "    for e in range(epoch):\n",
    "        model, optimizer, train_loss = train(train_loader, model,\n",
    "                                criterion, optimizer, device)\n",
    "\n",
    "\n",
    "\n",
    "        # for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #     # data, target = data, target\n",
    "        #     optimizer.zero_grad()\n",
    "        #     output = model(data)\n",
    "        #     loss = F.nll_loss(output, target) # The negative log likelihood loss\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "    # return loss.item()\n",
    "    print(train_loss)\n",
    "    return train_loss\n",
    "\n",
    "def server_aggregate(global_model, client_models):\n",
    "    \"\"\"\n",
    "    This function has aggregation method 'mean'\n",
    "    \"\"\"\n",
    "    ### This will take simple mean of the weights of models ###\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "        print(global_dict[k])\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n\n",
    "    \n",
    "def test(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    for X, y_target in valid_loader:\n",
    "        # if there is a GPU\n",
    "\n",
    "        X = X.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "\n",
    "        # prediction and loss\n",
    "\n",
    "        # call model forward()\n",
    "        y_predict, _ = model(X)\n",
    "        # get loss\n",
    "        loss = criterion(y_predict, y_target)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "    return model, epoch_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### training #######\n",
    "###### List containing info about learning #########\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "acc_train = []\n",
    "acc_test_arr = []\n",
    "\n",
    "for r in range(num_rounds):\n",
    "    # select random clients\n",
    "    # we select in the total number of clients, a random array of clients of size num_selected at each round\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    \n",
    "    loss = 0 # why 0?\n",
    "    for i in range(num_selected):\n",
    "        loss += client_update(federatedModels[i], optimizers[i], train_loader[client_idx[i]],DEVICE,criterion, epoch=EPOCHS)\n",
    "        train_acc = get_accuracy(federatedModels[i], train_loader[client_idx[i]], DEVICE)\n",
    "\n",
    "        # print(train_loader[i])\n",
    "        print(train_acc)\n",
    "        print(loss)\n",
    "    losses_train.append(loss)\n",
    "    server_aggregate(centralizedModel, federatedModels)\n",
    "\n",
    "    test_loss= test(test_loader,centralizedModel,criterion, DEVICE) # Test global model on data\n",
    "    test_acc = get_accuracy(centralizedModel, test_loader, DEVICE)\n",
    "    \n",
    "    # print(test_acc)\n",
    "    losses_test.append(test_loss)\n",
    "    acc_test_arr.append(test_acc)\n",
    "    print('%d-th round' % r)\n",
    "\n",
    "    # print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, test_acc))\n",
    "    print('average train loss %0.3g ' % (loss / num_selected))\n",
    "    print(test_loss)\n",
    "    print(type(test_loss))\n",
    "    print(' test loss %0.3g '%(test_loss))\n",
    "    print('test acc: %0.3f'% (test_acc))\n",
    "\n",
    "\n",
    "    # train_acc = get_accuracy(model, train_loader, device)\n",
    "    # test_acc = get_accuracy(model, test_loader, device)\n",
    "    # DOES NOT GET THE ACCURACY, CHECK WITH THE FUNCITON FROM THE FEDERATED LEARNING AAND COMPARE WITH CENTRALISED MODEL"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a605e0aa9ba0153468378976782e6a438b220bce716ba7043b32bc2b74e9bc8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
